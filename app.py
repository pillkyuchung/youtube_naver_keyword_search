# app.py
# -*- coding: utf-8 -*-
from __future__ import annotations

import os
import json
from datetime import datetime, date, timezone
import datetime as dt

import pandas as pd
import streamlit as st
from googleapiclient.discovery import build

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ê³µí†µ: ENV/í‚¤ ë¡œë”©
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from _env import Naver_CLient_ID, Client_Secret
NAVER_CLIENT_ID = Naver_CLient_ID
NAVER_CLIENT_SECRET = Client_Secret
NAVER_DATALAB_URL = "https://openapi.naver.com/v1/datalab/search"

import _env as _env_mod
YOUTUBE_API_KEY = _env_mod.API['youtube_api']

st.set_page_config(page_title="ë°ì´í„° ë„êµ¬ ëª¨ìŒ (Naver DataLab / YouTube)", page_icon="ğŸ“Š", layout="wide")
st.title("ğŸ“Š ë°ì´í„° ë„êµ¬ ëª¨ìŒ")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ê³µí†µ: ìœ í‹¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_resource(show_spinner=False)
def get_youtube_client(api_key: str):
    """googleapiclient ResourceëŠ” resource ìºì‹œì— ë³´ê´€"""
    return build("youtube", "v3", developerKey=api_key)

def iso_range_last_months(months: int) -> tuple[str, str]:
    """ì˜¤ëŠ˜ ê¸°ì¤€ ìµœê·¼ Nê°œì›”ì˜ ISO8601 êµ¬ê°„ ìƒì„± (UTC Z)"""
    today = date.today()
    start_dt = (pd.Timestamp(today) - pd.DateOffset(months=months)).date()
    after = f"{start_dt.isoformat()}T00:00:00Z"
    before = f"{today.isoformat()}T23:59:59Z"
    return after, before

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Naver DataLab êµ¬í˜„ (ê¸°ì¡´ ë¡œì§ ê¸°ë°˜)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def parse_groups(text: str):
    """'ê·¸ë£¹ëª…: í‚¤ì›Œë“œ1, í‚¤ì›Œë“œ2' í˜•ì‹ì˜ ì—¬ëŸ¬ ì¤„ â†’ keywordGroups êµ¬ì¡°"""
    groups = []
    for line in (text or "").splitlines():
        line = line.strip()
        if not line or ":" not in line:
            continue
        name, kws = line.split(":", 1)
        name = name.strip()
        keywords = [k.strip() for k in kws.split(",") if k.strip()]
        if name and keywords:
            groups.append({"groupName": name, "keywords": keywords})
    return groups

@st.cache_data(show_spinner=False, ttl=600)
def fetch_datalab(start_date: dt.date, end_date: dt.date, time_unit: str, keyword_groups: list[dict]) -> pd.DataFrame:
    """Naver DataLab ê²€ìƒ‰ íŠ¸ë Œë“œ API í˜¸ì¶œ â†’ DataFrame(period, ratio, title)"""
    if not keyword_groups:
        return pd.DataFrame()

    payload = {
        "startDate": start_date.strftime("%Y-%m-%d"),
        "endDate": end_date.strftime("%Y-%m-%d"),
        "timeUnit": time_unit,
        "keywordGroups": keyword_groups,
    }
    data = json.dumps(payload).encode("utf-8")

    req = st.experimental_singleton.clear  # no-op placeholder to avoid linter complaints
    import urllib.request
    req = urllib.request.Request(NAVER_DATALAB_URL)
    req.add_header("X-Naver-Client-Id", NAVER_CLIENT_ID)
    req.add_header("X-Naver-Client-Secret", NAVER_CLIENT_SECRET)
    req.add_header("Content-Type", "application/json")

    with urllib.request.urlopen(req, data=data, timeout=15) as resp:
        if resp.getcode() != 200:
            raise RuntimeError(f"HTTP {resp.getcode()}")
        body = resp.read().decode("utf-8")
        parsed = json.loads(body)

    frames = []
    for res in parsed.get("results", []):
        title = res.get("title")
        rows = res.get("data", [])
        if not rows:
            continue
        df = pd.DataFrame(rows)  # period, ratio
        df["title"] = title
        frames.append(df)

    if not frames:
        return pd.DataFrame()

    out = pd.concat(frames, ignore_index=True)
    out["period"] = pd.to_datetime(out["period"])
    return out

def render_naver_datalab():
    st.header("ğŸ” Naver DataLab ê²€ìƒ‰ íŠ¸ë Œë“œ")
    with st.sidebar:
        st.subheader("Naver DataLab ì„¤ì •")
        if not NAVER_CLIENT_ID or not NAVER_CLIENT_SECRET:
            st.info("`.streamlit/secrets.toml` ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ë¡œ NAVER í‚¤ë¥¼ ì„¤ì •í•˜ì„¸ìš”.")

        today = dt.date.today()
        default_start = today - dt.timedelta(days=60)
        start_date = st.date_input("ì‹œì‘ì¼", value=default_start, key="ndl_start")
        end_date = st.date_input("ì¢…ë£Œì¼", value=today, key="ndl_end")
        time_unit = st.selectbox("ì‹œê°„ ë‹¨ìœ„", ["date", "week", "month"], index=0, key="ndl_unit")

        st.markdown("**í‚¤ì›Œë“œ ê·¸ë£¹ ì…ë ¥ë²•**: í•œ ì¤„ì— í•œ ê·¸ë£¹, `ê·¸ë£¹ëª…: í‚¤ì›Œë“œ1, í‚¤ì›Œë“œ2`")
        st.caption("ì˜ˆ)  ì˜ì: ì˜ì, ì»´í“¨í„°ì˜ì\n      ì˜ì–´: ì˜ì–´, english")
        groups_raw = st.text_area("í‚¤ì›Œë“œ ê·¸ë£¹", value="ì˜ì: ì˜ì, ì»´í“¨í„°ì˜ì\nì˜ì–´: ì˜ì–´, english", height=120, key="ndl_groups")

        run_ndl = st.button("DataLab ì¡°íšŒ", key="ndl_run")

    if run_ndl:
        if not NAVER_CLIENT_ID or not NAVER_CLIENT_SECRET:
            st.error("NAVER_CLIENT_ID / NAVER_CLIENT_SECRETì´ í•„ìš”í•©ë‹ˆë‹¤.")
            return
        if start_date > end_date:
            st.error("ì‹œì‘ì¼ì´ ì¢…ë£Œì¼ë³´ë‹¤ í´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return

        groups = parse_groups(groups_raw)
        if not groups:
            st.warning("ì˜¬ë°”ë¥¸ í‚¤ì›Œë“œ ê·¸ë£¹ì„ ì…ë ¥í•˜ì„¸ìš”. ì˜ˆ) `ì˜ì: ì˜ì, ì»´í“¨í„°ì˜ì`")
            return

        try:
            df = fetch_datalab(start_date, end_date, time_unit, groups)
            if df.empty:
                st.warning("ë°ì´í„°ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ê¸°ê°„/í‚¤ì›Œë“œë¥¼ ë³€ê²½í•´ ë³´ì„¸ìš”.")
            else:
                st.subheader("ğŸ“Š ê²°ê³¼ (ë°ì´í„°í”„ë ˆì„)")
                st.dataframe(df.sort_values(["title", "period"]).reset_index(drop=True), use_container_width=True)
                st.line_chart(
                    df.pivot_table(index="period", columns="title", values="ratio", aggfunc="sum").sort_index()
                )
        except Exception as e:
            st.exception(e)
    else:
        st.info("ì‚¬ì´ë“œë°”ë¥¼ ì„¤ì •í•˜ê³  **DataLab ì¡°íšŒ**ë¥¼ ëˆ„ë¥´ì„¸ìš”.")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# YouTube êµ¬í˜„ (ìš”ì²­ ë°˜ì˜: ë‹¨ì¼ í‚¤ì›Œë“œ, ìµœê·¼ Nê°œì›” ì „ì²´ ìˆ˜ì§‘, ì¡°íšŒìˆ˜ 100 ì´í•˜ ì œê±°)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _videos_list_chunked(youtube, ids: list[str]) -> list[dict]:
    out = []
    for i in range(0, len(ids), 50):
        resp = youtube.videos().list(part="snippet,statistics", id=",".join(ids[i:i+50])).execute()
        out.extend(resp.get("items", []))
    return out

@st.cache_data(show_spinner=False, ttl=600)
def fetch_all_youtube(api_key: str, keyword: str, order: str,
                      published_after: str, published_before: str) -> list[dict]:
    """ê¸°ê°„ ë‚´ ëª¨ë“  ê²°ê³¼ í˜ì´ì§€ë„¤ì´ì…˜ ìˆ˜ì§‘ â†’ ìƒì„¸ ì¡°íšŒ"""
    youtube = get_youtube_client(api_key)

    all_ids: list[str] = []
    page_token = None
    while True:
        params = {
            "q": keyword,
            "part": "id",
            "type": "video",
            "order": order,
            "maxResults": 50,
            "publishedAfter": published_after,
            "publishedBefore": published_before,
        }
        if page_token:
            params["pageToken"] = page_token

        sr = youtube.search().list(**params).execute()
        ids = [it["id"]["videoId"] for it in sr.get("items", []) if it.get("id", {}).get("videoId")]
        all_ids.extend(ids)

        page_token = sr.get("nextPageToken")
        if not page_token:
            break

    if not all_ids:
        return []

    details = _videos_list_chunked(youtube, all_ids)
    videos = []
    for it in details:
        sn = it.get("snippet", {})
        stt = it.get("statistics", {})
        vid = it.get("id", "")
        videos.append({
            "title": sn.get("title", "") or "",
            "published_at": sn.get("publishedAt", "") or "",
            "url": f"https://www.youtube.com/watch?v={vid}",
            "view_count": int(stt.get("viewCount", 0)) if stt.get("viewCount") else 0,
            "like_count": int(stt.get("likeCount", 0)) if "likeCount" in stt else 0,
            "comment_count": int(stt.get("commentCount", 0)) if "commentCount" in stt else 0,
        })
    return videos

def calc_ratios(row: dict) -> tuple[float, float, datetime | None]:
    view = max(int(row.get("view_count", 0)), 0)
    like = max(int(row.get("like_count", 0)), 0)
    comt = max(int(row.get("comment_count", 0)), 0)
    like_ratio = round(like / view, 6) if view else 0.0
    comment_ratio = round(comt / view, 6) if view else 0.0
    pub_dt = datetime.fromisoformat(row["published_at"].replace("Z", "+00:00")) if row.get("published_at") else None
    return like_ratio, comment_ratio, pub_dt

def yt_to_dataframe(videos: list[dict]) -> pd.DataFrame:
    """í‘œì‹œìš© DataFrame: ì œëª©, URL, ê²Œì‹œì¼, ì¡°íšŒìˆ˜, ì¢‹ì•„ìš” ë¹„ìœ¨, ëŒ“ê¸€ ë¹„ìœ¨ (ì¡°íšŒìˆ˜ 100 ì´í•˜ ì œê±°)"""
    if not videos:
        return pd.DataFrame(columns=["title", "url", "published_at", "view_count", "like_ratio", "comment_ratio"])
    rows = []
    for v in videos:
        if v["view_count"] < 100:
            continue
        like_r, comt_r, pub_dt = calc_ratios(v)
        rows.append({
            "title": v["title"],
            "url": v["url"],
            "published_at": pub_dt,
            "view_count": v["view_count"],
            "like_ratio": like_r,
            "comment_ratio": comt_r,
        })
    return pd.DataFrame(rows)

def render_youtube():
    st.header("ğŸ“º YouTube í‚¤ì›Œë“œ ë¶„ì„ (ìµœê·¼ Nê°œì›”, ì¡°íšŒìˆ˜ 100 ì´ˆê³¼)")
    with st.sidebar:
        st.subheader("YouTube ì„¤ì •")
        if not YOUTUBE_API_KEY:
            st.info("`.streamlit/secrets.toml` ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ë¡œ YOUTUBE_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
        keyword = st.text_input("í‚¤ì›Œë“œ (1ê°œ)", value="í—ˆë¨¼ë°€ëŸ¬ ì—ì–´ë¡ ", key="yt_kw").strip()
        months_back = st.number_input("ìµœê·¼ Nê°œì›” (ìµœëŒ€ 24)", min_value=1, max_value=24, value=6, step=1, key="yt_months")
        order = st.selectbox("ì •ë ¬", ["date", "relevance", "viewCount"], index=0, key="yt_order")
        run_yt = st.button("YouTube ì¡°íšŒ", key="yt_run")

    if run_yt:
        if not YOUTUBE_API_KEY:
            st.error("YouTube API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            return
        if not keyword:
            st.error("í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
            return

        after_iso, before_iso = iso_range_last_months(months_back)

        try:
            with st.spinner("YouTube API í˜¸ì¶œ ì¤‘ (í˜ì´ì§€ë„¤ì´ì…˜)â€¦"):
                videos = fetch_all_youtube(
                    api_key=YOUTUBE_API_KEY,
                    keyword=keyword,
                    order=order,
                    published_after=after_iso,
                    published_before=before_iso,
                )

            df = yt_to_dataframe(videos)

            st.subheader("ğŸ“¦ ìˆ˜ì§‘ ê°œìš”")
            c1, c2, c3 = st.columns(3)
            c1.metric("í‚¤ì›Œë“œ", keyword)
            c2.metric("ìˆ˜ì§‘ëœ ì˜ìƒ ìˆ˜ (ì¡°íšŒìˆ˜ 100 ì´ˆê³¼)", len(df))
            c3.metric("ê¸°ê°„", f"{after_iso[:10]} ~ {before_iso[:10]}")

            st.subheader("ğŸ“„ ì˜ìƒ ëª©ë¡")
            if df.empty:
                st.info("ì¡°ê±´ì— ë§ëŠ” ì˜ìƒì´ ì—†ìŠµë‹ˆë‹¤.")
            else:
                df_view = df.sort_values("published_at", ascending=False).reset_index(drop=True)
                st.dataframe(df_view, use_container_width=True)

                @st.cache_data
                def to_csv_bytes(xdf: pd.DataFrame) -> bytes:
                    return xdf.to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig")

                st.download_button(
                    "CSV ë‹¤ìš´ë¡œë“œ",
                    data=to_csv_bytes(df_view),
                    file_name=f"youtube_{keyword}_{datetime.now().strftime('%Y%m%d')}.csv",
                    mime="text/csv"
                )
        except Exception as e:
            st.exception(e)
    else:
        st.info("ì‚¬ì´ë“œë°”ë¥¼ ì„¤ì •í•˜ê³  **YouTube ì¡°íšŒ**ë¥¼ ëˆ„ë¥´ì„¸ìš”.")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë©”ì¸: ê¸°ëŠ¥ ì„ íƒ (íƒ­/ì…€ë ‰íŠ¸)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
mode = st.radio("ê¸°ëŠ¥ ì„ íƒ", ["Naver DataLab", "YouTube í‚¤ì›Œë“œ"], horizontal=True)

if mode == "Naver DataLab":
    render_naver_datalab()
else:
    render_youtube()
